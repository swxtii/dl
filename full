#full

import numpy as np
import pandas as pd

# -----------------------------
# 1. Dataset
# -----------------------------
# Hardcoded sample dataset (XOR problem)
X = np.array([[0,0],
              [0,1],
              [1,0],
              [1,1]])

y = np.array([[0],
              [1],
              [1],
              [0]])

# -----------------------------
# Example: Load dataset from Excel instead of hardcoding
# -----------------------------
# import pandas as pd
# from sklearn.model_selection import train_test_split

# df = pd.read_excel("data.xlsx")              # Load Excel file
# X = df.iloc[:, :-1].values                  # All columns except last = features
# y = df.iloc[:, -1].values.reshape(-1, 1)    # Last column = target

# -----------------------------
# Split into Train and Test sets
# -----------------------------
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, test_size=0.2, random_state=42
# )

# -----------------------------
# 2. Activation Functions
# -----------------------------
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Alternative: ReLU (commented)
# def relu(x):
#     return np.maximum(0, x)
# def relu_derivative(x):
#     return (x > 0).astype(float)

# -----------------------------
# 3. Weight Initialization (Xavier)
# -----------------------------
input_layer_size = X.shape[1]   # Number of features
hidden_layer_size = 4
output_layer_size = 1

np.random.seed(42)
W1 = np.random.randn(input_layer_size, hidden_layer_size) * np.sqrt(1. / input_layer_size)
b1 = np.zeros((1, hidden_layer_size))
W2 = np.random.randn(hidden_layer_size, output_layer_size) * np.sqrt(1. / hidden_layer_size)
b2 = np.zeros((1, output_layer_size))

# -----------------------------
# 4. Adam Optimizer Parameters
# -----------------------------
learning_rate = 0.01
epochs = 5000
beta1, beta2 = 0.9, 0.999
epsilon = 1e-8

# Initialize Adam moment estimates
mW1, vW1 = np.zeros_like(W1), np.zeros_like(W1)
mb1, vb1 = np.zeros_like(b1), np.zeros_like(b1)
mW2, vW2 = np.zeros_like(W2), np.zeros_like(W2)
mb2, vb2 = np.zeros_like(b2), np.zeros_like(b2)

# -----------------------------
# 5. Training Loop
# -----------------------------
for epoch in range(1, epochs+1):
    # Forward pass
    z1 = np.dot(X, W1) + b1
    a1 = sigmoid(z1)   # use relu(z1) if switching
    z2 = np.dot(a1, W2) + b2
    a2 = sigmoid(z2)

    # Loss (Mean Squared Error)
    loss = np.mean((y - a2) ** 2)

    # Backpropagation
    error_output = (a2 - y) * sigmoid_derivative(a2)
    error_hidden = np.dot(error_output, W2.T) * sigmoid_derivative(a1)

    dW2 = np.dot(a1.T, error_output)
    db2 = np.sum(error_output, axis=0, keepdims=True)
    dW1 = np.dot(X.T, error_hidden)
    db1 = np.sum(error_hidden, axis=0, keepdims=True)

    # Adam updates
    # Update biased moment estimates
    mW1 = beta1 * mW1 + (1 - beta1) * dW1
    vW1 = beta2 * vW1 + (1 - beta2) * (dW1 ** 2)
    mb1 = beta1 * mb1 + (1 - beta1) * db1
    vb1 = beta2 * vb1 + (1 - beta2) * (db1 ** 2)

    mW2 = beta1 * mW2 + (1 - beta1) * dW2
    vW2 = beta2 * vW2 + (1 - beta2) * (dW2 ** 2)
    mb2 = beta1 * mb2 + (1 - beta1) * db2
    vb2 = beta2 * vb2 + (1 - beta2) * (db2 ** 2)

    # Bias correction
    mW1_hat = mW1 / (1 - beta1 ** epoch)
    vW1_hat = vW1 / (1 - beta2 ** epoch)
    mb1_hat = mb1 / (1 - beta1 ** epoch)
    vb1_hat = vb1 / (1 - beta2 ** epoch)

    mW2_hat = mW2 / (1 - beta1 ** epoch)
    vW2_hat = vW2 / (1 - beta2 ** epoch)
    mb2_hat = mb2 / (1 - beta1 ** epoch)
    vb2_hat = vb2 / (1 - beta2 ** epoch)

    # Parameter updates
    W1 -= learning_rate * mW1_hat / (np.sqrt(vW1_hat) + epsilon)
    b1 -= learning_rate * mb1_hat / (np.sqrt(vb1_hat) + epsilon)
    W2 -= learning_rate * mW2_hat / (np.sqrt(vW2_hat) + epsilon)
    b2 -= learning_rate * mb2_hat / (np.sqrt(vb2_hat) + epsilon)

    # Print progress
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# -----------------------------
# 6. Test Predictions
# -----------------------------
print("Final Predictions:")
print(a2.round(3))


# -----------------------------
# After training, classify new (test) data
# -----------------------------
# z1_test = np.dot(X_test, W1) + b1
# a1_test = sigmoid(z1_test)
# z2_test = np.dot(a1_test, W2) + b2
# a2_test = sigmoid(z2_test)

# predictions_test = (a2_test > 0.5).astype(int)
# print("Test Predictions (0/1):", predictions_test.flatten())
# print("True Labels:", y_test.flatten())
